# -*- coding: utf-8 -*-
"""creditrisk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ts0atQyRkLYeTmf4IWnKaL6r6smhh_q
"""

# Commented out IPython magic to ensure Python compatibility.
#IMPORTING LIBRARIES

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score

# %matplotlib inline

#LOADING THE DATSET
df = pd.read_csv("/content/drive/MyDrive/bankloans.csv")
df.head()  #df.head() gives us the first 5 entries of the dataset

#checks in each columns how many null values are there
df.isnull().sum()

#count of unique rows in the datset
df.value_counts()

#drops all the rows where there is any unavailable value
df = df.dropna()

# create a figure and axis with a specified size (20 inches wide, 10 inches tall)
fig, ax = plt.subplots(figsize=(20,10))

# plot a line chart using Seaborn:
# x-axis: 'age' column from the DataFrame
# y-axis: 'income' column from the DataFrame
# data: DataFrame 'df'
# ax: use the Axes object we created earlier
sns.lineplot(x='age', y='income', data=df, ax=ax)

# create a figure and axis with a specified size (20 inches wide, 10 inches tall)
fig, ax = plt.subplots(figsize=(20,10))

# plot a line chart using Seaborn:
# x-axis: 'age' column from the DataFrame
# y-axis: 'debtinc' column from the DataFrame
# data: DataFrame 'df'
# ax: use the Axes object we created earlier
sns.lineplot(x='age', y='debtinc', data=df, ax=ax)

df['default'].value_counts()

# creating our training and test dataset
x=df.drop(['default'],axis=1)
y=df['default']

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42) #random_state value will make sure that the values provided on train and test are based on normal distribution, random_state value should be between 40 to 50 to work best for all the models.

# scaling the date
sc = StandardScaler()
xtrain = sc.fit_transform(xtrain)
xtest = sc.fit_transform(xtest)

"""# **Creating Model**

RANDOM FOREST
"""

rfc = RandomForestClassifier(n_estimators=200)  # n_estimators is the no. of trees i am going to use

# fit our data
rfc.fit(xtrain, ytrain)

# check the score and the score will be tested using the tested values
rfc.score(xtest, ytest)

"""0.8 = 80% of the score we are getting with random forest."""

#using cross validation score to find optimal score
rfc2 = cross_val_score(estimator=rfc, X=xtrain, y=ytrain, cv=10)
print(rfc2.mean())
# cv equals to how many times are we going to take this cross_val_score, cv=10 means 10 times you have to execute rfc.fit(xtrain,ytrain) take the mean of all those things and put them in the new model rf2.

"""78.39% optimal score from random forest.

SVM
"""

sv = SVC()
sv.fit(xtrain,ytrain)

sv.score(xtest,ytest)

model = GridSearchCV(sv, {
    'C': [0.1, 0.2, 0.4, 0.8, 1.2, 1.8, 4.0, 7.0],
    'gamma' : [0.1, 0.4, 0.8, 1.0, 2.0, 3.0],
    'kernel' : ['rbf', 'linear']
}, scoring='accuracy', cv=10)

model.fit(xtrain, ytrain)

model.best_params_

model2 = SVC(C=0.1, gamma=0.1, kernel='linear')
model2.fit(xtrain,ytrain)
model2.score(xtest,ytest)

"""LOGISTIC REGRESSION"""

lr = LogisticRegression()
lr.fit(xtrain,ytrain)

lr.score(xtest,ytest)

yp = lr.predict(xtest)
c = confusion_matrix(ytest,yp)
fig, ax = plt.subplots(figsize=(5,5))
sns.heatmap(c, ax=ax)
ax.set_xlabel("Predicted")
ax.set_ylabel("Actual")
ax.set_title("Confusion Matrix - Logistic Regression")